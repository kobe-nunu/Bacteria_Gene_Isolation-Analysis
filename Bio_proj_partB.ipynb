{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PGXynHpXVRl"
      },
      "source": [
        "# Bioinformatics project - part 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igSq6wY7XVRq"
      },
      "source": [
        "\n",
        "In this part we designed and wrote a program that does the following:\n",
        "\n",
        "<br>\n",
        "\n",
        "Given a set of COG-spelled genomes S, where each genome is segmented into segments such that each segment could contain one or more operons, and given parameters q, l and an unknown COG X, finds all strings c of length l that are conserved in at least q of the genomes in the database S, such the our unknown COG appears at least once in c.\n",
        "\n",
        "<br>\n",
        "\n",
        "The COG chosen by us is COG 0673.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Before You start"
      ],
      "metadata": {
        "id": "9K36jpk6tVUH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make sure you have the files \"cog_words_bac.csv\",\"cog_words_plasm.csv\" and \"cog_info_table.csv\" in your working directory for the script to run properly.\n",
        "\n",
        "\n",
        "#### Let's start!"
      ],
      "metadata": {
        "id": "rzHXJa0VtY88"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UoL4VRtoXVRv"
      },
      "source": [
        "## 1. Initializing parameters and imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_2UeitkWXVRw"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "urOE5Do0XVRy"
      },
      "outputs": [],
      "source": [
        "cog = '0673'\n",
        "q = 5\n",
        "l = 5 # we look for all strings c where  2 <= |c| <= 10 to start with and to get more information for the biological analysis\n",
        "k = 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5C4eN04oXVRz"
      },
      "source": [
        "## 2. Searching for all words with our cog"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fhxvKklkXVRz"
      },
      "outputs": [],
      "source": [
        "def search_words(database, cog):\n",
        "    lst = []\n",
        "    for index, row in database.iterrows():\n",
        "        if cog in row['E']:\n",
        "            lst.append([row['E'], row['B']])\n",
        "    return lst"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88OrkK4jXVR0"
      },
      "source": [
        " > search_words goes through every single line in our database and gathers all instances which include our cog. <br>\n",
        " We also record each organism where the line was presented for later use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VQCMotnUXVR2"
      },
      "outputs": [],
      "source": [
        "words_bact = pd.read_csv(\"./cog_words_bac.csv\") #loading the first database\n",
        "words_plasm = pd.read_csv(\"./cog_words_plasmid.csv\") #loading the second database\n",
        "\n",
        "bact_lst = search_words(words_bact, cog)\n",
        "plasm_lst = search_words(words_plasm, cog)\n",
        "full_lst = bact_lst + plasm_lst"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lv25ECNmXVR3"
      },
      "source": [
        "This is the \"heaviest\" function in our program, since it is looping over the entire database and looking for instances of our cog. <br>\n",
        "The time complexity is $O(|DATABASE|*|COG|)$, and since |cog|=4 which is constant the time complexity overall is $O(|DATABASE|)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1cKszLUXVR4"
      },
      "source": [
        "## 2.1 Encoding our list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkOyhYoBXVR5"
      },
      "source": [
        "Our list is now made of 2-items lists: in the first item(full_lst[i][0]) there is a word which contains our COG, and the second item(full_lst[i][1]) contatins the organism that had this word in its genome."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_H-edq_yXVR6",
        "outputId": "1be5ad83-6ea5-49ce-9896-fd71d9b16a6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['505\\t0079\\t0673\\t3475\\t1211\\t0451\\tX\\tX\\t0463\\t2244\\t1209\\t1088\\t1091\\t0463\\t1898\\t1088\\t', 'NC_016077']\n",
            "505\t0079\t0673\t3475\t1211\t0451\tX\tX\t0463\t2244\t1209\t1088\t1091\t0463\t1898\t1088\t\n",
            "NC_016077\n"
          ]
        }
      ],
      "source": [
        "print(full_lst[8])\n",
        "print(full_lst[8][0])\n",
        "print(full_lst[8][1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfYGdemzXVR7"
      },
      "source": [
        "As you can see, our words are made out of 3-4 digits \"letters\" which are seperated by tabs. in the next stage we want to extract all substrings of each word"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_KmO3MkXVR8"
      },
      "source": [
        "- Due to the sturcture of the words, it may be inconvenient to loop through the words and letters, so we decided to enconde each letter (which is actually an integer) into its ascii character. <br>\n",
        "- There are over 10,000 characters which are encoded in ascii, so each COG will be mapped to exactly one character in an injective way. <br>\n",
        "- That way we know we won't harm our words, and after removing the tabs we will get words just like we know them, which are easy to work with."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5IBy3GwpXVR9"
      },
      "outputs": [],
      "source": [
        "def encode(words):\n",
        "    coded_lst = []\n",
        "    for word in words:\n",
        "        organism = word[1]\n",
        "        word = word[0]\n",
        "        index = 0\n",
        "        right = 0\n",
        "        str = ''\n",
        "        while right != -1:\n",
        "            if word[index] == 'X':\n",
        "                str += 'X'\n",
        "                right = get_next(word, right)\n",
        "                index = right\n",
        "                continue\n",
        "            right = get_next(word, right)\n",
        "            ascii = int(word[index:right - 1])\n",
        "            index = right\n",
        "            char = chr(ascii)\n",
        "            str += char\n",
        "        coded_lst.append([str, organism])\n",
        "    return coded_lst\n",
        "\n",
        "def get_next(str, index):\n",
        "    len1 = len(str)\n",
        "    while str[index] != '\\t':\n",
        "        index += 1\n",
        "    index += 1\n",
        "    return index if index < len1 else -1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04em_sQRXVR-"
      },
      "source": [
        " > The words might not be distinguishable to human eyes, but the computer can tell the differences due to the way Strings are stored in memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eq4fHHQ3XVR-",
        "outputId": "9698bda8-8eb1-49f9-d2ed-f5ee7bea5269"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ǹOʡඓһǃXXǏ\\u08c4ҹруǏݪl', 'NC_016077']\n"
          ]
        }
      ],
      "source": [
        "coded_lst = encode(full_lst)\n",
        "print(coded_lst[8])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uwDSweiXVR_"
      },
      "source": [
        "## 2.2 Getting all substrings with our COG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VT2v1vBSXVR_"
      },
      "source": [
        "Now that our list is ready to work with, we want to get all substrings in length l which contains our cog. <br>\n",
        "\n",
        "- *we won't forget to count each appearance of each substring, since multiple appearances might tell us something about this particular string.* <br>\n",
        "\n",
        "- *we also won't forget to keep the organism in which each substring was in, for the next section.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OcGMayukXVSA"
      },
      "outputs": [],
      "source": [
        "def get_subs(str, counter, length, organism):\n",
        "    for i in range(len(str)):\n",
        "        sub = str[i: i + length]\n",
        "        if len(sub) == length and chr(int(cog)) in sub:\n",
        "            if sub in counter:\n",
        "                counter[sub][0] += 1\n",
        "                counter[sub][2].add(organism)\n",
        "            else:\n",
        "                counter[sub] = [1, len(sub), set()]\n",
        "                counter[sub][2].add(organism)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBBAV4kZXVSA"
      },
      "source": [
        "This is a simple program to find substrings. For a String with size s, we want to generate all substrings with length l which contains our COG.<br>\n",
        "\n",
        "This is being done in $O(l*|S|+|S|) = O(l*|S|)$ for each string."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DrZSJtniXVSB"
      },
      "outputs": [],
      "source": [
        "counter = {}\n",
        "for j in range(len(coded_lst)): #loop over every string in our list\n",
        "    for i in range(2, 11): #get all strings with lengths 2 to 10\n",
        "        get_subs(coded_lst[j][0], counter, i, coded_lst[j][1])\n",
        "\n",
        "        #The assignment askes us to find only substrings with length l, but we found all substrings in lengths 2 to 10. \n",
        "        #To do the assignment specifically, instead of running in range 2,11 like we did you can just call get_subs with i=l:\n",
        "        #for j in range(len(coded_lst)):\n",
        "            #get_subs(coded_lst[j][0], counter, l, coded_lst[j][1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAC_2ArAXVSJ"
      },
      "source": [
        "#### Worst case scenerio, our coded_lst size (which contains only the words with our COG in it) is the same size as our database (in reality, |coded_lst| <<< |Database|). <br>\n",
        "\n",
        " Lets consider the longest word in the database as s. Since in the worst case, |coded_lst| = |Database|, we are calling \"get_subs\" |Database| times. <br>\n",
        "\n",
        "> Each \"get_subs\" call computes in $O(l*|s|)$ time, where |s| is the length of the longest word in the database. <br>\n",
        "\n",
        "> Each insertion to counter, which is a python dictionary, which is actually implemented as a hash map, is made from a search($O(1)$ approximately in hash maps) and an insertion($O(1)$).\n",
        "\n",
        "> So worst case scenerio, the overall runtime of the algorithm up to this point is $O(|Database| * |COG| * |s| * l) ≈≈ O(|Database| * |s| * l)$. <br>\n",
        "\n",
        "> Since l < s (otherwise there won't be any words with length l), and since the size of the longest word in the database is a constant, $O(|Database| * |s| * l) = O(|Database|)$ <br>\n",
        "\n",
        "Overall, the time complexity is $O(|Database|)$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDjhHBjmXVSJ"
      },
      "source": [
        "#### As for the space complexity, after searching through all the database we keep all the relevant words in a list on memory. <br>\n",
        "\n",
        "- Worst case scenerio, this list is |Database| long, so space complexity for this list is $O(|Database|)$.\n",
        "\n",
        "- After that we are encoding the list, meaning we are reducing its size to a $1/4$ of its original size. $O(1/4 * |Database|) = O(|Database|)$.\n",
        "\n",
        "- Then we are creating a dictionary which contains all substrings with length l. There are $l*|Database|$ such substrings, each substring is with length l, for a total of $O(|l^2 * |Database| + |Database|) = O(|l^2 * |Database|)$ space needed.\n",
        "\n",
        "- Since l is a constant, $O(|l^2 * |Database|) = O(|Database|)$.\n",
        "\n",
        "- Rest of the filtering and analysis will only reduce space needed, so total space needed is $O(|Database|)$.\n",
        "\n",
        "> In Reality, since the first list is much smaller than the size of the database, the actual space needed is much smaller than |Database|. <br>\n",
        "_For reference, you can see that the Database's size is approximately 60k kb, and our csv output weighs ≈ 500kb_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTMCKap4XVSK"
      },
      "source": [
        "### 2.3 Filtering according to q parameter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKHfq7XYXVSK"
      },
      "source": [
        "Now that we have all substrings kept in a dictionary, it's time to keep only the substrings which appeared on at least q different organisms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tgFqIGNXVSK"
      },
      "source": [
        "if we take a look in our dictionary, we can see it's made of keys, which are our substrings, and values, which are now 3-items lists:\n",
        "- counter['sub'][0] contains the number of times we witnessed this particular sub. <br>\n",
        "\n",
        "- counter['sub'][1] contains the length of the substring\n",
        "\n",
        "- counter['sub'][2] contains a set of all the genomes where this substring appeared."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xEFUwdv8XVSM",
        "outputId": "52ef0aab-3eb4-4833-c5f8-176790bd3c8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 4, {'NC_013209'}]\n"
          ]
        }
      ],
      "source": [
        "print(counter['ॢʡÑ\\x14'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzhoJLhXXVSM"
      },
      "source": [
        "> As you can see, despite the fact this substring seems to in length 7, it actually has only 4 cogs in it. \n",
        "\n",
        "\n",
        "> We can also see it appears in only one COG. since our q value is 5, we want to filter it out."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "1R8X5JdeXVSN"
      },
      "outputs": [],
      "source": [
        "counter = {k: v for k, v in counter.items() if len(v[2]) > q} # filtering according to q value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IQhIR-hcXVSN",
        "outputId": "d255e456-f157-4c94-84a5-c3566f86072e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n"
          ]
        }
      ],
      "source": [
        "print('ॢʡÑ\\x14' in counter)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XM2faessXVSO"
      },
      "source": [
        "> As mentioned in the previous section, |counter| = O(|Database|). Checking whether len(counter[s][2] > 2) is done in O(1), so overall this step takes O(|Database|) worse case scenerio.\n",
        "\n",
        "> Of course, since |counter| <<< |Database|, this steps takes much less time in reality."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2N7L01yXVSO"
      },
      "source": [
        "# Enforcing 'k' Constraint:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LaSDeCHSXVSO"
      },
      "source": [
        "We say that 𝑆𝑖,1≤ 𝑖 ≤ 𝑛, is a k-instance of 𝑝 if there exists a string 𝑌 such that 𝑌 is a substring \n",
        "of 𝑆𝑖  and 𝑌 can be obtained from 𝑝 by inserting up to 𝑘 characters to 𝑝.  \n",
        "\n",
        "After finding all substring lists in length 2<len<10, the genomes they appear in and the number of their occurences, we needed to find all k-instances for each of the substrings.\n",
        "\n",
        "To enforce that, we used 2 helper methods:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WCr8TgBxXVSO"
      },
      "outputs": [],
      "source": [
        "def stringDiff(sub, cand):\n",
        "    \n",
        "    if ((len(cand) > len(sub) + k) or (len(cand) < len(sub))):\n",
        "        return False\n",
        "    if lcs(sub, cand) < len(sub):\n",
        "        return False\n",
        "    \n",
        "    return True\n",
        "    \n",
        "def lcs(text1, text2):\n",
        "    dp = [[0] * (len(text2) + 1) for _ in range(len(text1) + 1)]\n",
        "    for i, c in enumerate(text1):\n",
        "        for j, d in enumerate(text2):\n",
        "            dp[i + 1][j + 1] = 1 + dp[i][j] if c == d else max(dp[i][j + 1], dp[i + 1][j])\n",
        "    return dp[-1][-1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8S_tA07XVSP"
      },
      "source": [
        "> stringDiff takes two strings(sub, cand) and returns True if cand is a k-instance of sub.\n",
        "\n",
        "> it is done in two parts:\n",
        "* Checking the length of both strings and determining if it's even logically possible.\n",
        "* Using the second helper method \"lcs\"(longest common subsequence)\n",
        "\n",
        "> lcs is a dynamic programming method which computes the longest common subsequence between the two input strings, as shown in class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKacPypHXVSP"
      },
      "source": [
        "Using those two methods, we went over our substring container and for each two substrings, checked wether or not they are k-instances of each other and updated the data accordingly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rGIsuz9xXVSP"
      },
      "outputs": [],
      "source": [
        "new_counter = {}\n",
        "\n",
        "for sub in counter.keys():\n",
        "    occ2 = 0\n",
        "    Org_set = set()\n",
        "    for cand in counter.keys():\n",
        "        if (stringDiff(sub, cand)):\n",
        "            occ2 += counter[cand][0]\n",
        "            Org_set.update(counter[sub][2])\n",
        "    new_counter[sub] = [occ2, counter[sub][1], Org_set]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_R6PlevXVSQ"
      },
      "source": [
        "## Time Complexity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFqrdxemXVSQ"
      },
      "source": [
        "> counter.keys() has $O|Database|$ values worst case scenerio(in reallity it's much smaller)\n",
        "\n",
        "> for each value in counter.keys we compute stringDiff for every value in counter.keys()\n",
        "\n",
        "> So overall, time complexity is $O(|Database|^2) * O(stringDiff)$\n",
        "\n",
        "> stringDiff is calling $O(1)$ operations and lcs(). lcs is being computed in $O(|s1|*|s2|)$ as shown in class.\n",
        "\n",
        "> consider L as the length of the longest substring in the text.\n",
        "\n",
        "> overall time complexity is $O(|Database|^2 * L^2)$\n",
        "\n",
        "> In our case, $ 2 < L < 10 $, theredore our time complexity is $O(|Database|^2)$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMTPaA5LXVSQ"
      },
      "source": [
        "## Space Complexity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFgc6PSrXVSQ"
      },
      "source": [
        "> Our dictionary stays about the same size, so $O(|database|)$ worse case just like before.\n",
        "\n",
        "> lcs creates 2-D matrix each run, but frees the memory as soon as the lcs activation frame is closed.\n",
        "\n",
        "> Overall, lcs uses at most $O(L^2)$ in space.\n",
        "\n",
        "> Therefore overall space complexity is $O(|database|)$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkRCLG_UXVSQ"
      },
      "source": [
        "#### Now that we have filtered out the unnecessary strings, we can decode our strings!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aEFy4s8OXVSR"
      },
      "outputs": [],
      "source": [
        "def decode(counter):\n",
        "    dict = {}\n",
        "    for key in counter.keys():\n",
        "        res = ''\n",
        "        for char in key:\n",
        "            code = str(ord(char))\n",
        "            code = (4 - len(code))*'0'+code\n",
        "            res += code\n",
        "            res += '\\t'\n",
        "        dict[res] = counter[key]\n",
        "    return dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ww_5KFDSXVSR"
      },
      "outputs": [],
      "source": [
        "counter = decode(counter)\n",
        "new_counter = decode(new_counter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s7tXqBY-XVSR"
      },
      "outputs": [],
      "source": [
        "#uncomment and run the next line of code in order to filter and get only subs with length l\n",
        "#new_counter = {k: v for k, v in counter.items() if v[1] == l}\n",
        "#however, we won't do this for now, since we want all substrings with length 2 to 10 for a more comprehensive biological analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3BLZPM2XVSS"
      },
      "source": [
        "### 2.4 Issuing a csv report with all the information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y4SZH03QXVSS"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "new_dict = {k: v for k, v in sorted(new_counter.items(), key=lambda item: len(item[1][2]), reverse = True)}\n",
        "\n",
        "df2 = pd.DataFrame.from_dict(new_dict, orient='index').reset_index()\n",
        "df2 = df2.rename(columns={'index': 'word', 1: 'length', 0: 'occurrences', 2: 'organisms'})\n",
        "s = 'reportB_' + cog + '.csv'\n",
        "df2.to_csv(f\"./{s}\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOCOTiBqXVSS"
      },
      "source": [
        "Now you can see a csv report with all the information is available in the directory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCDITGraXVSS"
      },
      "source": [
        "> For better presentation, it is recommended to download the csv file and open it on your computer.\n",
        "\n",
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ss9TMUOxXVST"
      },
      "source": [
        "### Bonus stage: analysing the csv file for a better representation of our results\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSFnLv98XVST"
      },
      "source": [
        "Now that we have all our relevant data ready, we want to present each word made of cogs and align it with their known functionality"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sFzeT6vYXVST"
      },
      "outputs": [],
      "source": [
        "def analyze(cog):\n",
        "    df = pd.read_csv(f\"./reportB_{cog}.csv\")\n",
        "    new_df = df[df['length'] >= 5] #we want to take only long strings with our cog\n",
        "    results = []\n",
        "    for index, row in new_df.iterrows(): #splitting the tabs from our strings\n",
        "        cogs = row['word'][:-1]\n",
        "        cog_list = cogs.split('\\t')\n",
        "        results.append([cog_list, row['occurrences']])\n",
        "        \n",
        "    cog_table = pd.read_csv(\"./cog_info_table.csv\") #this table contains the functionality of each cog, so we are loading it for now.\n",
        "\n",
        "    to_remove = []\n",
        "    for result in results: #now we want to remove all sequences which are substrings of other sequences. This is for better visualisation\n",
        "        for result2 in results:\n",
        "            if all(elem in result[0] for elem in result2[0]) and result[0] != result2[0] and result[0] not in to_remove:\n",
        "                to_remove.append(result2[0])\n",
        "                break\n",
        "\n",
        "    results = [x for x in results if x[0] not in to_remove]\n",
        "    with open(f\"./final_report_{cog}.txt\", \"w\") as report:\n",
        "        for lst in results:\n",
        "            lst_to_str = '\\t'.join(lst[0]) + '\\t'\n",
        "            ls = str(lst[0]) + f\"    ||||||||     num of occurances: {lst[1]}\\n\"\n",
        "            flag = False\n",
        "            for cog2 in lst[0]:\n",
        "                d = cog_table.loc[cog_table['A'] == 'COG{}'.format(cog2), 'D'].values\n",
        "                e = cog_table.loc[cog_table['A'] == 'COG{}'.format(cog2), 'E'].values\n",
        "                s = d + '|' + e\n",
        "                try:\n",
        "                    if 'Translation' in s[0] or 'translation' in s[0]: #filtering results related to translation\n",
        "                        i = 0\n",
        "                        flag = True\n",
        "                    else:\n",
        "                        ls = ls + s + '\\n'\n",
        "                except IndexError:\n",
        "                    print(f'cog_{cog2} is faulty')\n",
        "                    ls = ls + f'Unknown_{cog2}\\n'\n",
        "            if not flag:\n",
        "                report.write(''.join(ls))\n",
        "    report.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oxu7eaHYXVSU"
      },
      "outputs": [],
      "source": [
        "analyze(cog)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vS6Kbz-qXVSU"
      },
      "source": [
        "## Now you can open \"final_report_{cog}\" in the directory and start the biological analysis!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "Bio_proj_partB",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
